---
title: "Robots.txt"
date: 2020-02-15T12:52:36+06:00
image_webp: images/blog/blog-post-1.webp
image: images/blog/blog-post-1.png
author: Сайткомп
description : "Как влияет robots.txt на индексацию сайта"
---

### Как влияет robots.txt на индексацию сайта

Файл robots.txt является одним из самых важных при оптимизации любого сайта. Его отсутствие может привести к высокой нагрузке на сайт со стороны поисковых роботов и медленной индексации и переиндексации, а неправильная настройка к тому, что сайт полностью пропадет из поиска или просто не будет проиндексирован. Следовательно, не будет искаться в Яндексе, Google и других поисковых системах. Давайте разберемся во всех нюансах правильной настройки robots.txt.


Поисковые роботы будут индексировать ваш сайт независимо от наличия файла robots.txt. Если же такой файл существует, то роботы могут руководствоваться правилами, которые в этом файле прописываются. При этом некоторые роботы могут игнорировать те или иные правила, либо некоторые правила могут быть специфичными только для некоторых ботов. В частности, GoogleBot не использует директиву Host и Crawl-Delay, YandexNews с недавних пор стал игнорировать директиву Crawl-Delay, а YandexDirect и YandexVideoParser игнорируют более общие директивы в роботсе (но руководствуются теми, которые указаны специально для них).

> #### Подробнее об исключениях:

#### [Исключения Яндекса](https://yandex.ru/support/webmaster/controlling-robot/robots-txt.html#exceptions)

#### [Стандарт исключений для роботов (Википедия)](https://ru.wikipedia.org/wiki/%D0%A1%D1%82%D0%B0%D0%BD%D0%B4%D0%B0%D1%80%D1%82_%D0%B8%D1%81%D0%BA%D0%BB%D1%8E%D1%87%D0%B5%D0%BD%D0%B8%D0%B9_%D0%B4%D0%BB%D1%8F_%D1%80%D0%BE%D0%B1%D0%BE%D1%82%D0%BE%D0%B2)


`Максимальную нагрузку` на сайт создают роботы, которые скачивают контент с вашего сайта. Следовательно, указывая, что именно индексировать, а что игнорировать, а также с какими временны́ми промежутками производить скачивание, вы можете, с одной стороны, значительно снизить нагрузку на сайт со стороны роботов, а с другой стороны, ускорить процесс скачивания, запретив обход ненужных страниц.

К таким ненужным страницам относятся скрипты `ajax`, `json`, отвечающие за всплывающие формы, баннеры, вывод каптчи и т.д., формы заказа и корзина со всеми шагами оформления покупки, функционал поиска, личный кабинет, админка.

Для большинства роботов также желательно отключить индексацию всех JS и CSS. Но для GoogleBot и Yandex такие файлы нужно оставить для индексирования, так как они используются поисковыми системами для анализа удобства сайта и его ранжирования [пруф Google](https://webmasters.googleblog.com/2014/10/updating-our-technical-webmaster.html)  и  [пруф Яндекс](https://webmaster.yandex.ru/blog/21369).

> Директивы robots.txt
***

Директивы — это правила для роботов. Есть спецификация W3C от 30 января 1994 года и расширенный стандарт от 1996 года. Однако не все поисковые системы и роботы поддерживают те или иные директивы. В связи с этим для нас полезнее будет знать не стандарт, а то, как руководствуются теми или иными директивы основные роботы.

##### User-agent
Это самая главная директива, определяющая для каких роботов далее следуют правила.

Для всех роботов:
`User-agent: *`

Для конкретного бота:
`User-agent: GoogleBot`

Обратите внимание, что в robots.txt `не важен регистр` символов. Т.е. юзер-агент для гугла можно с таким же успехом записать соледующим образом:
user-agent: googlebot

> Ниже приведена таблица основных юзер-агентов различных поисковых систем.

***

| Бот | Функция |
| ------ | ------ |
| Google |
|Googlebot-News             |Google Новости|
|Googlebot-Image            |Google Картинки|
|Googlebot-Video            |видео|
|Googlebot                  |основной индексирующий робот Google|
|Mediapartners-Google       |Google AdSense, Google Mobile AdSense|
|Mediapartners              |Google AdSense, Google Mobile AdSense|
|AdsBot-Google              |проверка качества целевой страницы|
|AdsBot-Google-Mobile-Apps  |Google для приложений|